{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/dask_horizontal.svg\"\n",
    "     width=\"45%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "     \n",
    "# Schedulers\n",
    "\n",
    "Dask collections like Dask DataFrames and Dask Delayed objects are used to build up task graphs for a computation. After these graphs are generated, they need to be executed (potentially in parallel). This is the job of a Dask task scheduler. Different task schedulers exist within Dask. Each will consume a task graph and compute the same result, but with potentially different performance characteristics. \n",
    "\n",
    "Dask has two different classes of schedulers: single-machine schedulers and a distributed scheduler.\n",
    "\n",
    "![grid-search](images/grid_search_schedule.gif \"grid-search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Machine Schedulers\n",
    "\n",
    "Single machine schedulers provide basic features on a local process or thread pool and require no setup (only use the Python standard library). The different single machine schedulers Dask provides are:\n",
    "\n",
    "- `\"threads\"`: The threaded scheduler executes computations with a local `multiprocessing.pool.ThreadPool`. The threaded scheduler is the default choice for Dask arrays, Dask DataFrames, and Dask delayed. \n",
    "\n",
    "- `\"processes\"`: The multiprocessing scheduler executes computations with a local `multiprocessing.Pool`.\n",
    "\n",
    "- `\"single-threaded\"`: The single-threaded synchronous scheduler executes all computations in the local thread, with no parallelism at all. This is particularly valuable for debugging and profiling, which are more difficult when using threads or processes.\n",
    "\n",
    "You can configure which scheduler is used in a few different ways. You can set the scheduler globally by using the `dask.config.set(scheduler=)` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.datasets import timeseries\n",
    "\n",
    "df = timeseries(start=\"2020-11-11\", end=\"2020-11-15\", partition_freq=\"10min\")\n",
    "result = df[[\"x\", \"y\"]].resample(\"1h\").mean()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "dask.config.set(scheduler='threads')\n",
    "result.compute(); # Will use the multi-threading scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or use it as a context manager to set the scheduler for a block of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.config.set(scheduler='processes'):\n",
    "    result.compute()  # Will use the multi-processing scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or even within a single compute call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.compute(scheduler='threads');  # Will use the multi-threading scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `num_workers` argument is used to specify the number of threads or processes to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.compute(scheduler='threads', num_workers=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Scheduler\n",
    "\n",
    "Despite having \"distributed\" in it's name, the distributed scheduler works well on both single and multiple machines. Think of it as the \"advanced scheduler\".\n",
    "\n",
    "The Dask distributed cluster is composed of a single centralized scheduler and one or more worker processes. A `Client` object is used as the user-facing entry point to interact with the cluster.\n",
    "\n",
    "<img src=\"images/dask-cluster.svg\"\n",
    "     width=\"85%\"\n",
    "     alt=\"Dask components\\\">\n",
    "     \n",
    "\n",
    "Deploying a remote Dask cluster involves some [additional setup](https://distributed.dask.org/en/latest/setup.html). There are several projects for easily deploying a Dask cluster on commonly used computing resources:\n",
    "\n",
    "- [Dask-Kubernetes](https://kubernetes.dask.org/en/latest/) for deploying Dask using native Kubernetes APIs\n",
    "- [Dask-Yarn](https://yarn.dask.org/en/latest/) for deploying Dask on YARN clusters\n",
    "- [Dask-MPI](http://mpi.dask.org/en/latest/) for deploying Dask on existing MPI environments\n",
    "- [Dask-Jobqueue](https://jobqueue.dask.org/en/latest/) for deploying Dask on job queuing systems (e.g. PBS, Slurm, etc.)\n",
    "\n",
    "Setting up the distributed scheduler locally just involves creating a `Client` object, which lets you interact with the \"cluster\" (local threads or processes on your machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: when we create a distributed scheduler `Client`, by default it registers itself as the default Dask scheduler. All `.compute()` calls will automatically start using the distributed scheduler unless otherwise told to use a different scheduler. \n",
    "\n",
    "The distributed scheduler has many features:\n",
    "\n",
    "- [Sophisticated memory management](https://distributed.dask.org/en/latest/memory.html)\n",
    "\n",
    "- [Data locality](https://distributed.dask.org/en/latest/locality.html)\n",
    "\n",
    "- [Adaptive deployments](https://distributed.dask.org/en/latest/adaptive.html)\n",
    "\n",
    "- [Cluster resilience](https://distributed.dask.org/en/latest/resilience.html)\n",
    "\n",
    "- ...\n",
    "\n",
    "See the [Dask distributed documentation](https://distributed.dask.org) for full details about all the distributed scheduler features.\n",
    "\n",
    "For this talk, I'd like to highlight two of the distributed scheduler features: real time diagnostics and the futures interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Futures interface\n",
    "\n",
    "The Dask distributed scheduler implements a superset of Python's [`concurrent.futures`](https://docs.python.org/3/library/concurrent.futures.html) interface that allows for finer control and asynchronous computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def inc(x):\n",
    "    time.sleep(0.5)\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    time.sleep(0.5)\n",
    "    return 2 * x\n",
    "\n",
    "def add(x, y):\n",
    "    time.sleep(0.5)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run these functions locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can submit them to run remotely on a Dask worker node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = client.submit(inc, 1)\n",
    "future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Client.submit` function sends a function and arguments to the distributed scheduler for processing. It returns a `Future` object that refer to remote data on the cluster. The `Future` returns immediately while the computations run remotely in the background. There is no blocking of the local Python session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wait a moment, and then check on the future again, you'll see that it has finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can retrieve the result of a `Future` by calling the `.result()` method. If the status of the `Future` is \"finished\", meaning the task has been successfully run on one of the workers, then calling `.result()` will return almost immediately. Conversely, if the `Future` is still \"pending\" then calling `.result()` will block the current Python process and wait until the task has been run and then return the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `Client.submit`, there's also a `Client.map` function for running a function across an interable of inputs (similar to Python's built in `map` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to doing: [client.submit(inc, i) for i in range(10)]\n",
    "futures = client.map(inc, list(range(10)))\n",
    "futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Client.map` returns a list of `Future` objects, one for each of the inputs being mapped over. To get the results for mutliple futures, you can use the `Client.gather` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to doing: [f.result() for f in futures]\n",
    "results = client.gather(futures)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Future`s obey standard Python garbage collection. That is, the data `Future`s point to will continue to live on a Dask worker until there are no more references to the `Future`, at which point they will be deleted from the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del futures[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying task dependencies\n",
    "\n",
    "Much like the `dask.delayed` interface, we can submit new tasks that depend on the results of other futures. This will create a dependency between the inputs and outputs. Dask will track the execution of all tasks and ensure that downstream tasks are run at the proper time and place and with the proper data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = client.submit(inc, 20)\n",
    "y = client.submit(double, 5)\n",
    "z = client.submit(add, x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Parallelize a for-loop\n",
    "\n",
    "For this exercise, we'll repeat Exercise 1 from the Dask Delayed notebook, but this time instead of using `dask.delayed` use the Distributed scheduler's `Future`s interface to parallelize the `for`-loop below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "def inc(x):\n",
    "    time.sleep(0.5)\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    time.sleep(0.5)\n",
    "    return 2 * x\n",
    "\n",
    "def add(x, y):\n",
    "    time.sleep(0.5)\n",
    "    return x + y\n",
    "\n",
    "data = list(range(10))\n",
    "\n",
    "output = []\n",
    "for x in data:\n",
    "    a = inc(x)\n",
    "    b = double(x)\n",
    "    c = add(a, b)\n",
    "    output.append(c)\n",
    "\n",
    "total = sum(output)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/schedulers-1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun Example: Dynamic computations\n",
    "\n",
    "In the [Delayed notebook](1-delayed.ipynb), we saw that `Delayed` objects can't be used control flow (e.g. `if`/`else`), it's difficult to create dynamic workflows. For example, a workflow where whether or not subsequent tasks are submitted depends on the output of previous tasks. The `Future`s interface provides more flexibility in these situations where computations may evolve over time.\n",
    "\n",
    "For this, we can use operations like [`as_completed()`](https://distributed.dask.org/en/latest/api.html#distributed.as_completed), which returns futures in the order in which they complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import as_completed\n",
    "\n",
    "# Create some initial set of Futures\n",
    "futures = []\n",
    "for i in range(64):\n",
    "    x = client.submit(inc, i)     # x = inc(i)\n",
    "    y = client.submit(double, x)  # y = inc(x)\n",
    "    z = client.submit(add, x, y)  # z = inc(y)\n",
    "    futures.append(z)\n",
    "\n",
    "# Use as_completed to create an iterator which yields\n",
    "# Futures as the complete\n",
    "seq = as_completed(futures)\n",
    "while seq.count() > 2:  # at least two futures left\n",
    "    a = next(seq)\n",
    "    b = next(seq)\n",
    "    new = client.submit(add, a, b)  # add them together\n",
    "    seq.add(new)                    # add new future back into as_completed iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a brief demo of the distributed scheduler. It's has lots of other cool features not touched on here. For more information, check out the [Distributed documentation](https://distributed.dask.org). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources\n",
    "\n",
    "- Dask links:\n",
    "\n",
    "    - GitHub repository: https://github.com/dask/dask\n",
    "\n",
    "    - Documentation: https://docs.dask.org\n",
    "\n",
    "    - Dask examples repository: https://github.com/dask/dask-examples\n",
    "\n",
    "- There are lots of great Dask tutorial from various conference on YouTube. For example:\n",
    "\n",
    "    - \"Parallelizing Scientific Python with Dask\" @ SciPy 2018: [YouTube](https://www.youtube.com/watch?v=mqdglv9GnM8)\n",
    "    \n",
    "    - \"Scalable Machine Learning with Dask\" @ SciPy 2018: [YouTube](https://www.youtube.com/watch?v=ccfsbuqsjgI)\n",
    "\n",
    "- If you have a Dask usage questions, please ask it on [Stack Overflow with the #dask tag](https://stackoverflow.com/questions/tagged/dask). Dask developers monitor this tag and will answer questions.\n",
    "\n",
    "- If you run into a bug, feel free to file a report on the [Dask GitHub issue tracker](https://github.com/dask/dask/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
